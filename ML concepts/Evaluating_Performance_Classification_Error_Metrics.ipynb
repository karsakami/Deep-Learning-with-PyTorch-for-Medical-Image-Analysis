{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrPQSuWmMzwwoIBcw2S6NO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karsakami/Deep-Learning-with-PyTorch-for-Medical-Image-Analysis/blob/main/ML%20concepts/Evaluating_Performance_Classification_Error_Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluating Performance for classification problems.\n",
        "\n",
        " After learning process is complete, we will use performance metrics to evaluate how our model did on test set\n",
        "\n",
        "Key classification metrics:\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precision\n",
        "*   F1-Score\n",
        "\n",
        "But lets understand a reason, why we use these metrics \n",
        "when classification task your model can actually only achieve two results? - well or bad.\n",
        "\n",
        "Lets simplify our example, we will use a binary classification and predict if an image is a dog or cat. \n",
        "\n",
        "We need to train model on training data, our mission is to fit model to our true data as much as possible, since we know answers, bcs its supervised learning.\n",
        "\n",
        "Then test model on test data and evaluate performance. For example just input a dog picture, we compare model prediction to the correct label and we count correct matches and incorrect\n"
      ],
      "metadata": {
        "id": "ehk9tahxFw8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##So, in a nutshell:\n",
        "- We use all images in our X test data\n",
        "- We get a count of a incorrect and correct predictions and compare them\n",
        "\n",
        "*W prawdziwym świecie nie wszystkie niepoprawne lub poprawne matche maja taka sama wartosc*\n",
        "\n",
        "We can organize our predicted values compared to the real values in confusion matrix"
      ],
      "metadata": {
        "id": "qeNT3zfFURwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Accuracy** is a very simple metrics and its correct predictions made by the model divided by total number of predictions. Is answers to the question: among all the instances, how many did we classify correctly?\n",
        "\n",
        "###**Accuracy** is useful when target classes are well balanced, that mean if we have a same amout of cat images as we have dog images, labels are equally represented in the dataset. But its not a good choice with unbalanced classes, for example if we have 1 image of cat and 99 images of dogs.\n",
        "\n",
        "*W takim przypadku, jeśli model caly czas przewiduje psa, to pomyli sie tylko raz. Jeśli mamy np model klasyfikacji binarnej, który przewiduje spam lub nie, to nie bedzie wielkim problemem, jesli spam trafi do skrzynki odbiorczej, ale wiekszy problem bedzie jesli wazna wiadomosc trafi do spamu. \n",
        "Podobnym przypadkiem może być model służący do klasyfikacji guzów jako złośliwych lub łagodnych. Dosłownie istotnym błędem jest wykrycie złośliwego guza jako łagodnego. Nasz model powinien skupić się bardziej na poprawnym wykrywaniu guzów złośliwych.*\n",
        "\n",
        "Accuracy can be easily evaluated in scikit-learn using the accuracy_score() function."
      ],
      "metadata": {
        "id": "Sh1vaRywUSxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_true, y_pred)"
      ],
      "metadata": {
        "id": "D3K12lilmAhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#So when we have unbalanced classes, we want to understand **recall** and **precision** metrics\n",
        "\n",
        "###**Recall** = $\\frac{true.positives}{true.positives+false.negatives}$ \n",
        "\n",
        "\n",
        "###**Precision** = $\\frac{true.postivies}{true.positives + false.positives}$\n",
        "\n",
        "precision is the answer to the question: out of all the data points predicted to be positive, how many of them were actually positive? \n",
        "In other words, when the model classifies a data point as positive, it is correct 75% of the time. The higher the precision, the more confident we can be that our model output is correct.\n"
      ],
      "metadata": {
        "id": "rDGG7-eNhRK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision and recall can be combined into a single metric called  **F1  score**, which is the harmonic mean between **precision and recall**.\n",
        " \n",
        "An  F1  score reaches its best value at 1. That value tends to be closer to the smallest one."
      ],
      "metadata": {
        "id": "Qm5YUkpO0U8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So whats good accuracy? It depends. Depends on situation, context"
      ],
      "metadata": {
        "id": "sYhDJBm82iT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jeśli mamy doczyniania z poważnym problemem to musimy mieć jak najwieksza dokladnosc modelu"
      ],
      "metadata": {
        "id": "uz_slVmT4aXV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nCFbwXPRVIJ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}